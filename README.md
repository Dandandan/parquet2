# Parquet2

This is an experiment, a re-write of the parquet crate that
* delegates parallelism downstream
* deletages decompressing and decoding batches downstream
* no `unsafe`

## Organization

* `read`: read metadata and pages
* `metadata`: parquet files metadata (e.g. `FileMetaData`)
* `schema`: types metadata declaration (e.g. `ConvertedType`)
* `types`: physical type declaration (i.e. how things are represented in memory). So far unused.
* `compression`: compression (e.g. Gzip)
* `errors`: basic error handling
* `serialization`: convert from bytes to typed buffers (`Vec<T>` or Arrow's buffers).

## Run tests

We run some tests against files generated by `pyarrow==3`, which are generated by developers
locally. To generate, run

```bash
python3 -m venv venv
venv/bin/pip install -r requirements.txt
venv/bin/python -m write_parquet
```

The files are located in `fixtures/`, which is ignored by git.

Afterwards, run `cargo test` as usual.

## How to use

```rust
use std::fs::File;

use parquet2::read::{Page, read_metadata, get_page_iterator};

let mut file = File::open("testing/parquet-testing/data/alltypes_plain.parquet").unwrap();

/// here we read the metadata.
let metadata = read_metadata(&mut file)?;

/// Here we get an iterator of pages (each page has its own data)
/// This can be heavily parallelized; not even the same `file` is needed here...
/// feel free to wrap `metadata` under an `Arc`
let row_group = 0;
let column = 0;
let mut iter = get_page_iterator(&metadata, row_group, column, &mut file)?;

/// A page. It is just (compressed) bytes at this point.
let page = iter.next().unwrap().unwrap();
println!("{:#?}", page);

/// from here, we can do different things. One of them is to convert its buffers to native Rust.
/// This consumes the page.
use parquet2::serialization::native::page_to_array;
let array = page_to_array(page, &descriptor).unwrap();

/// Alternatively, we can convert the page to an arrow2 array
use parquet2::serialization::arrow2::page_iter_to_array;
let array = page_iter_to_array(iter).unwrap();
```

There are many types that are not supported atm. The plan is to add support to them.

### Higher Parallelism

The function above creates an iterator over a row group, `iter`. In arrow, this corresponds 
to a `RecordBatch`, divided in Parquet pages. Typically, converting a page into in-memory is expensive
and thus consider how to distribute work across threads. E.g.

```rust 
let handles = vec![];
for column in columns {
    let compressed_pages = get_page_iterator(&metadata, row_group, column, &mut file, file)?.collect()?;
    // each compressed_page has a buffer; cloning is expensive(!). We move it so that the memory
    // is released at the end of the processing.
    handles.push(thread::spawn move {
        page_iter_to_array(compressed_pages.into_iter())
    })
}
let columns_from_all_groups = handles.join_all();
```

this will read the file as quickly as possible in the main thread and send CPU-intensive work to other threads, thereby maximizing IO reads (at the cost of storing multiple compressed pages in memory).

## General data flow

`parquet -> decompress -> decode -> deserialize`

* `decompress`: e.g. `gzip`
* `decode`: e.g. `RLE`
* `deserialize`: e.g. `&[u8] -> &[i32]`
